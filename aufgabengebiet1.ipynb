{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabengebiet 1\n",
    "\n",
    "## Aufgabenstellung\n",
    "\n",
    "* Mustererkennung und Segmentierung, um bestimmte Bereiche auf einem Katalogzettel seiner Funktion (z.B. Signaturnummer) zuzuordnen\n",
    "* Training und Anpassung von OCR-Software (z.B. Tesseract OCR) aufgrund der Mehrsprachigkeit und der teils handschriftlichen Beschriftung der Zettel\n",
    "* Plausibilitätsüberprüfung der zu erwartenden OCR-Ergebnisse durch den Abgleich mit bibliothekarischen Normdaten (z.B. Existenz des Komponistennamens)\n",
    "* Speicherung dieser Daten in maschinenlesbaren Form\n",
    "\n",
    "### Beispielhafte Karteikarten\n",
    "![Karteikarte](http://musikipac.staatsbibliothek-berlin.de/ipac_musik/imgdat/SD/SD002/JPG/00000171.jpg)\n",
    "[Der Kompletteintrag im IPAC findet sich hier.](http://musikipac.staatsbibliothek-berlin.de/ipac_musik/catalog/main?cn=SD&lin=SD0010502&rin=SD0020171&css=11)\n",
    "\n",
    "Im Bestand finden sich jedoch auch noch handschriftliche Karteikarten, denen man sich im weiteren Verlauf des Projekts widmen sollte. Da hier die OCR-Erkennungsleistung einbrechen wird, empfiehlt es sich, dass zuerst eine Toolchain für maschinengeschriebene Karten entworfen und implementiert wird und man sich dann den komplizierten Materialien widmet.\n",
    "\n",
    "![Karteikarte, handschriftlich](http://musikipac.staatsbibliothek-berlin.de/ipac_musik/imgdat/S/S001/JPG/00000027.jpg)\n",
    "[Der Kompletteintrag im IPAC findet sich hier.](http://musikipac.staatsbibliothek-berlin.de/ipac_musik/catalog/main?cn=S&lin=S0010001&rin=S0010026&ro=1&css=11&cop=:osy)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR-Werkzeuge\n",
    "\n",
    "* Wie bereits gesagt, ist die Erkennung von Handschriften eine besondere Herausforderung, wie [hier](http://www.dasbibliothekswissen.de/Computer-liest-historische-Handschriften.html?from=ONL-BIBLIO&wa=BIB15N-36&srnr=SR11063494&uid=4759719) auch sehr gut zusammengefasst wird. Bei dem besprochenen Werkzeug handelt es sich um [__Transkribus__](https://transkribus.eu/Transkribus/). Das Werkzeug sollten wir in jedem Fall evaluieren und dann können wir gerne mit Günter Mühlberger (dem Forschungskoordinator) Kontakt aufnehmen, da wir bereits mit ihm zusammenarbeiten.\n",
    "\n",
    "* Ein Art Platzhirsch im Opensource-OCR ist [__Tesseract__](https://github.com/tesseract-ocr). In einem kurzen [Artikel](http://www.heise.de/open/artikel/Toolbox-Texterkennung-mit-Tesseract-OCR-1674881.html) wird dessen Funktionsweise gut umrissen. Tesseract muss generell trainiert werden, um gute Ergebnisse zu erzielen. Wie das geht, steht [hier](https://code.google.com/p/tesseract-ocr/wiki/TrainingTesseract3) und [hier](https://groups.google.com/forum/#!topic/tesseract-ocr/5pixYQr2kQM).\n",
    "\n",
    "* In letzter Zeit bekam auch [__OCRopus__](https://github.com/tmbdev/ocropy) mehr Aufmerksamkeit. Es kann verschiedene Texterkennungmodule einbinden, wobei zur Zeit nur Tesseract zur Verfügung steht. Nichtsdestotrotz erzielt es in Kombination mit Tesseract bessere Ergebnisse, als wenn man Tesseract alleine nutzt ([vgl. Wikipedia](https://de.wikipedia.org/wiki/OCRopus)). Vielleicht kann man es ja mit Transkribus kombinieren?\n",
    "\n",
    "* Desweiteren gibt es noch [GNU OCR](http://www.gnu.org/software/ocrad/) und [OCR Feeder](https://wiki.gnome.org/action/show/Apps/OCRFeeder?action=show&redirect=OCRFeeder), ein GUI-Tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plausibilitätsprüfung\n",
    "\n",
    "* Ein wesentliches Schlagwort, um die Plausibilität eines Eintrags zu erkennen ist [__Named Entity Recognition__](https://en.wikipedia.org/wiki/Named-entity_recognition). Hier finden sich im Web einige Ressourcen. Die Herausforderung hier ist die Mehrsprachigkeit der Materialien bzw. die teilweise sehr speziellen Inhalte.\n",
    "* Um an validierte Aussagen im Musikbereich zu kommen, bietet sich u.a. [__MusicBrainz__](https://musicbrainz.org/) an.\n",
    "* Außerdem bietet [__Wikidata__](https://wikidata.org) eine Vielzahl an Fakten, die man per [SPARQL](https://de.wikipedia.org/wiki/SPARQL) abfragen kann.\n",
    "    * https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries\n",
    "    * https://www.mediawiki.org/wiki/Wikidata_query_service/User_Manual\n",
    "    * https://www.mediawiki.org/wiki/Wikibase/Indexing/SPARQL_Query_Examples\n",
    "* [__Freebase__](https://www.freebase.com/) ist mittlerweile in Wikidata aufgegangen, jedoch sind dessen [Dumps](https://developers.google.com/freebase/data) nach wie vor verfügbar.\n",
    "* Last but not least, kann man die [__GND__](http://www.dnb.de/DE/Standardisierung/GND/gnd_node.html), die Gemeinsame Normdatei, per [OAI-PMH anfragen](http://www.dnb.de/DE/Service/DigitaleDienste/OAI/oai_node.html). Siehe dazu auch mein [Tutorial](https://github.com/elektrobohemian/dst4l-copenhagen/blob/master/oai_test.ipynb).\n",
    "* Alternativ kann man mittels [__Clustering-Verfahren__](https://de.wikipedia.org/wiki/Clusteranalyse) oder [__Klassifizierung__](https://de.wikipedia.org/wiki/Klassifizierung) Muster in den erzeugten Texten erkennen, um z.B. OCR- oder Rechtschreibfehler zu erkennen. Siehe dazu auch meine Tutorials zur [Klassifizierung](https://github.com/elektrobohemian/dst4l-copenhagen/blob/master/NaiveBayes.ipynb) und zum [Clustering](https://github.com/elektrobohemian/dst4l-copenhagen/blob/master/ClusteringTextAnalysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verschiedenes\n",
    "\n",
    "* Um die Erkennungsrate zu erhöhen, muss man sich der unter Umständen [Spracherkennung](https://en.wikipedia.org/wiki/Language_identification) widmen, wenn die Sprache der Karten nicht als Metadatum vorliegt.\n",
    "* Alchemy bietet WebServices für eine Vielzahl an [Anwendungen](http://blog.alchemyapi.com/topic/recipes), die mitunter interessant für das Projekt sein könnten. Einen API-Key für 1.000 Transaktionen bekommt man umsonst.\n",
    "* Ein einfacher Algorithmus, um falsch erkannte Worte zu erkennen ist die [Levenshtein-Distanz](https://de.wikipedia.org/wiki/Levenshtein-Distanz).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
